{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase-Aware Speech Enhancement with Deep Complex U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной стате рассматривается подход к очищению данных от шума при помощи модифицированной сети\n",
    "U-Net - DCUnet.\n",
    "\n",
    "Также, помимо модификации сети U-net, авторы предложили новый подход для получения маски (complex ratio mask (cRM)) и loss функцию.\n",
    "\n",
    "## 1. Deep Complex U-Net\n",
    "\n",
    "Для получения маски авторы предлагают использовать модифицорованную сеть U-Net.\n",
    "В данной сети авторы используют комплекснозначные нейронные слои, представленные\n",
    "в статье Trabelsi и др. «Deep Complex Networks».\n",
    "Нейронная сеть применяется к данным, преобразованным через STFT.\n",
    "\n",
    "Моя реализация находится в классе `src.model.dcunet.DCUnet`.\n",
    "\n",
    "![alt text](../img/dcunet10.png )\n",
    "\n",
    "\n",
    "### 1.1. Complex Convolution layer\n",
    "\n",
    "Вместо обычных конволюционных слоев в DCUnet используется комплексные конволюции.\n",
    "Определяется она следующим образом:\n",
    "имеем фильтр $W = A + i B$, где $A$ и $B$ - матрицы из реальных значений. Применение данного\n",
    "фильтра к вектору $h = x + i y$ определяется как $W h = (A x - B y) + i (A x + B y)$.\n",
    "\n",
    "Моя реализация находится в классе `src.model.complexnet.ComplexConv2d`. Так же реализована деконволюция `src.model.complexnet.ComplexConvTranspose2d`.\n",
    "\n",
    "![alt text](../img/cconv.png)\n",
    "\n",
    "### 1.2. $\\mathbb{C}\\mathrm{ReLU}$\n",
    "\n",
    "В качетве функции активации используется $\\mathbb{C}\\mathrm{ReLU}$:\n",
    "$$\n",
    "\\mathbb{C}\\mathrm{ReLU}(x + i y)= \\mathrm{ReLU}(x) + i \\mathrm{ReLU}(y).\n",
    "$$\n",
    "\n",
    "Моя реализация находится в классе `src.model.complexnet.ComplexReLU`. В реализции она представляет просто ReLU активацию.\n",
    "\n",
    "### 1.3. Complex Batch normalization\n",
    "\n",
    "Нормирование реализовано как перемножение 0-центрированных данных\n",
    "на обратный корень матрицы ковариации 2x2:\n",
    "$$\n",
    "\\tilde{x} = (V)^{-\\frac{1}{2}} (x - \\mathbb{E}(x))\n",
    "$$\n",
    "где\n",
    "$$\n",
    "V = \\begin{pmatrix}\n",
    "V_{rr} & V_{ri} \\\\\n",
    "V_{ir} & V_{ii}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "В реализации имеем $V_{\\mathrm{run}}$, $E_{\\mathrm{run}}$\n",
    "для нормализации при вычислении тестовых данных. $V_{ii}$ и $V_{rr}$ инициализируются как\n",
    "$\\frac{1}{\\sqrt{2}}$. Во время тренировки они сдвигаются в сторону полученных V и E на тестовых данных\n",
    "с параметром magnitude.\n",
    "\n",
    "Так же как и в обычной batch нормализации используется два обучающих параметра $\\beta \\in \\mathbb{C}$ \n",
    "и $\\gamma$ - матрица 2x2. Тогда комплексная batch нормализация определяется как:\n",
    "$$\n",
    "\\mathrm{BN} = \\gamma x + \\beta\n",
    "$$\n",
    "\n",
    "Начальные значения для параметров имеем следующие: $\\gamma_{ii}$ и $\\gamma_{rr}$ как $\\frac{1}{\\sqrt{2}}$,\n",
    "$\\beta = 0 + i 0$.\n",
    "\n",
    "Моя реализация находится в классе `src.model.complexnet.ComplexBatchNorm2d`.\n",
    "\n",
    "## 2. cRM\n",
    "\n",
    "![alt text](../img/crm.png)\n",
    "\n",
    "Оцененная речевая спектрограмма вычисляется умножением\n",
    "оценочной маски $M_{t,f}$ на входной спектрограмме $X_{t,f}$ следующим образом:\n",
    "$$\n",
    "\\hat{Y}_{t,f}  = \\hat{M}_{t,f}  \\cdot \\hat{X}_{t,f}\n",
    "$$\n",
    "\n",
    "Для получения маски авторы предлагают подход, основанный на полярных координатах. \n",
    "Маска получается следующим образом:\n",
    "$$\n",
    "\\hat{M}_{t,f} = |\\hat{M}_{t,f}| \\cdot e^{i \\theta_{\\hat{M}_{t,f}}} = \\hat{M}^{\\mathrm{mag}}_{t,f} \\cdot \\hat{M}^{\\mathrm{phase}}_{t,f}\n",
    "$$\n",
    "$$\n",
    "\\hat{M}^{\\mathrm{phase}}_{t,f} = \\frac{ O_{t,f} }{ |O_{t,f}|}\n",
    "$$\n",
    "$$\n",
    "\\hat{M}^{\\mathrm{mag}}_{t,f} = \\tanh{( |O_{t,f}|)}\n",
    "$$\n",
    "где $O_{t,f}$ - выход сети.\n",
    "\n",
    "## 3. Loss функция\n",
    "\n",
    "Обычно в качестве loss функции используется MSE между чистым и предсказаным значением в STFT пространстве.\n",
    "Но в таком случе сигнал имеет не очень хорошую структуру.\n",
    "Авторы предлагают следующую функцию, которая учитывает соотношение шума и голоса, и используют только сигналы без\n",
    "STFT преобразования:\n",
    "$$\n",
    "\\mathrm{loss}_{wSDR}(x, y \\bar{y}) := \\alpha \\mathrm{loss}_{SDR}(y, \\bar{y}) +(1- \\alpha)\\mathrm{loss}_{SDR}(z, \\bar{z}) \n",
    "$$\n",
    "где $z$ - исходный шум, $\\bar{z} = x - \\bar{y}$,\n",
    "$$\n",
    "\\mathrm{loss}_{SDR}(y \\bar{y}) := -\\frac{<y, \\bar{y}>}{||y|| ||\\bar{y}||}\n",
    "$$\n",
    "и\n",
    "$$\n",
    "\\alpha = \\frac{||y||}{||y|| + ||z||}\n",
    "$$\n",
    "\n",
    "\n",
    "## 3. Система устранения шума\n",
    "\n",
    "В итоге можно собрать все в одну систему для устраненя шума. Сначала зашумленные данные\n",
    "преобразуются при помощи STFT. Далее проходят через DCUnet и при помощи cRM получается маска и\n",
    "выходное представление. Полученное представление преобразуется через ISTFT и получается \n",
    "звук без шума.\n",
    "\n",
    "\n",
    "Моя реализация находится в классе `src.model.enhmodel.EnhModel`.\n",
    "\n",
    "\n",
    "![alt text](../img/enh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты\n",
    "\n",
    "## Авторы\n",
    "Авторы статьи получили следующие результаты в сравнении с другими моделями:\n",
    "![alt text](../img/results.png)\n",
    "\n",
    "## Мои эксперемент\n",
    "\n",
    "В качестве данных для чистого голоса была взята выборка из набора данных LIBRISPEECH, а в качестве шума \n",
    "использовались записи из набора данных DEMAND.\n",
    "\n",
    "Все файлы имеют sample rate 42kHz. Для обучение он был уменьшен до 16kHz.\n",
    "\n",
    "Зашумление происходило так: чистый голос был объединён с 4-мя шумами с соотношением сигнал-шум (SNR) 15, 10, 5 или 0 dB.\n",
    "\n",
    "В качестве тренировочной выборки использовались 50 спикеров и 8 типов шумов.\n",
    "В качестве тестовой выборки использовались 5 спикеров и 2 типа шума.\n",
    "\n",
    "В каждой эпохе обучалось по $20 \\cdot 10$ случайных зашумлённых голосов и $10$ тестировалось.\n",
    "\n",
    "\n",
    "***Примечание:*** из-за нехватки вычеслительных ресурсов я провел эксперемент только на данных DCUnet10 и не смог провести достаточно вычислений для получения результата, сравнимого с результатами авторов, но прогресс обучения прослеживается. Также наверное я взял слишком много тестовых и тренеровочных данных.\n",
    "\n",
    "![alt text](../img/train_history.png)\n",
    "\n",
    "**Значение PESQ:** 2.3644541573524473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "Была реализована система отделения шума в речи на основе DCUnet.\n",
    "\n",
    "Проверена работоспособность и проведён эксперимент.\n",
    "\n",
    "Сложно сказать о качестве модели, основываясь на моих результатах.\n",
    "\n",
    "В дальнейшем возможно улучшить данную модель, например, использовав resnet блоки вместо U-net, или \n",
    "попробовать повысить робастность выходного слоя при помощи дополнительных регуляризаций.\n",
    "\n",
    "На практике данная модель может применяться в улучшении качества аудиосвязи при плохом качестве передачи\n",
    "или при разговоре в зашумлённых помещениях."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
